
> 写在开始：鉴于 MQ 在 SOA 架构中的重要地位，MQ 的性能和稳定性对整个系统而言举足轻重；本文针对线上近期遇到的 MQ 消息积压问题进行深入浅出的分析，旨在规范 MQ 的正确使用姿势，保障系统的稳定性；


# 原因描述（已简化）

- 由于框架升级，导致 queue 绑定规则多出一个万能匹配的 '#' 字段，进而导致多个 queue 出现消息堆积
- 发现生产者协议帧有错；
- queue 中消息大量堆积对集群中其他 queue 产生影响；

# 环境信息

> 以下信息为后续获取的，但应该和当时情况是对应的；

- RabbitMQ 状态信息

```shell
[root@xg-fanout-rmq-5 ~]# rabbitmqctl status
...
{rabbit,"RabbitMQ","3.2.4"},
...
 {os,{unix,linux}},
 {erlang_version,
     "Erlang R16B03-1 (erts-5.10.4) [source] [64-bit] [smp:24:24] [async-threads:30] [hipe] [kernel-poll:true]\n"},
...
 {vm_memory_high_watermark,0.6},
 {vm_memory_limit,19894665216},
...
[root@xg-fanout-rmq-5 ~]# 
```

- 集群信息

```
[root@xg-fanout-rmq-5 ~]# rabbitmqctl cluster_status
Cluster status of node 'rabbit@xg-fanout-rmq-5' ...
[{nodes,[{disc,['rabbit@xg-fanout-rmq-1','rabbit@xg-fanout-rmq-2',
                'rabbit@xg-fanout-rmq-3','rabbit@xg-fanout-rmq-4',
                'rabbit@xg-fanout-rmq-5','rabbit@xg-fanout-rmq-6',
                'rabbit@xg-fanout-rmq-7']}]},
 {running_nodes,['rabbit@xg-fanout-rmq-1','rabbit@xg-fanout-rmq-4',
                 'rabbit@xg-fanout-rmq-6','rabbit@xg-fanout-rmq-2',
                 'rabbit@xg-fanout-rmq-7','rabbit@xg-fanout-rmq-3',
                 'rabbit@xg-fanout-rmq-5']},
 {partitions,[]}]
...done.
[root@xg-fanout-rmq-5 ~]#
```

- queue 分布信息

```
[root@xg-fanout-rmq-5 ~]# rabbitmqctl -n rabbit@xg-fanout-rmq-5 -p zeus_fanout list_queues name arguments pid
...
base_openapi_queue_order	[]	<'rabbit@xg-fanout-rmq-7'.1.12945.3703>
...
osc_bacchus_queue	[]	<'rabbit@xg-fanout-rmq-5'.1.28523.744>
...
osc_hestia_queue_order	[]	<'rabbit@xg-fanout-rmq-7'.1.29064.6093>
...
pcd_marketing_queue_order_action	[]	<'rabbit@xg-fanout-rmq-5'.1.26695.3557>
...
[root@xg-fanout-rmq-5 ~]#
```

# 监控数据

从 `system.monitor.rabbitmq.queues`  [面板](https://t.elenet.me/dashboard/dashboard/db/system-monitor-rabbitmq-queues?from=1488432674328&to=1488454375317&var-vhost=zeus_fanout&var-name=All)中，可以看到消息积压最多的 3 个 queue 的情况；


![zeus_fanout_all_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/zeus_fanout_all_1.png "zeus_fanout_all_1")

![zeus_fanout_all_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/zeus_fanout_all_2.png "zeus_fanout_all_2")

![zeus_fanout_all_3](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/zeus_fanout_all_3.png "zeus_fanout_all_3")


> 问题：是不是 # 绑定规则仅针对如下 3 个 queue ；

## vhost:zeus_fanout + queue:pcd_marketing_queue_order_action

监控曲线：[这里](https://t.elenet.me/dashboard/dashboard/db/system-monitor-rabbitmq-queues?from=1488432674328&to=1488454375317&var-vhost=zeus_fanout&var-name=pcd_marketing_queue_order_action)；

![pcd_marketing_queue_order_action_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/pcd_marketing_queue_order_action_1.png "pcd_marketing_queue_order_action_1")

![pcd_marketing_queue_order_action_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/pcd_marketing_queue_order_action_2.png "pcd_marketing_queue_order_action_2")

![pcd_marketing_queue_order_action_3](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/pcd_marketing_queue_order_action_3.png "pcd_marketing_queue_order_action_3")

读图：

- 从 messages total 图中可以看到，消息从 14:20 左右开始积压，持续到 19:00 左右被 purge 掉，最高消息量 11 Mil；
- unacked 消息仅在 14:17 分和 14:29 分飙升到 20k 和 16k 左右，其余时间均在 200 左右；
- deliver 和 ack 速度在上述时间段，只有 100～3k 左右，平均 648 msg/s；
- publish 速度平均为 1.82k ，在 17:57 分左右瞬间飙到 420k 左右；
- memory 使用在上述时间段，呈现周期性梯形上涨（有 4～5 次掉底），达到 4.47 Bil 左右后会突降到 500 Mil 左右；
- consumer 数量在上述时间段为 200 个，经确认 consumer 的 prefetch count 设置为 1 ；
- consumer util 在上述时间段为 0 ；

分析：

- `200 consumer * 1 prefetch_count = 200 unacked msg` ，说明 unacked 曲线是合理的；
- `(1.82k publish rate - 648 delivery_get rate) * 3600 = 4.219 M` ，说明理论上（平均速度下）每小时消息堆积量会增加这么多，实际读图发现增量大概在 2.5M~3M msg/h 左右；
- delivery_get 曲线和 ack 曲线的平均速率在 648 msg/s ，在压测时间段内存在 200 consumer ，说明**每个 consumer 在 1 秒时间内处理了 3 条消息**（本人意见如下：在当前配置下，满载时 "in flight" 消息数量为 200 ，而当前消费者处理能力至少为 3 msg/s ，说明 prefetch_count 至少要大于 3 才合理，考虑到往返交互情况，prefetch_count 配置到 5-10 都不一定能令 consumer 满载；因此结论就是：业务 consumer 的处理能力被当前配置给限制住了；**具体情况需要业务确认是否合理**）；
- delivery_get 曲线和 ack 曲线吻合，说明 RabbitMQ 投递消息给 consumer 后收到 ack 很快，同时也说明 RabbitMQ 在收到 ack 后，再次投递消息给 consumer 也没有延迟；
- memory 曲线情况说明当消息大量积压并达到阈值后触发了 GC 行为；根据官方说明，Erlang GC 行为是 Erlang process 级别的，而每个 queue 均对应了一个 Erlang process ，因此可以确定，发生 GC 时必然会导致与目标 queue 相关的 producer 和 consumer 出现所谓的“速度上不去”问题；

## vhost:zeus_fanout + queue:osc_hestia_queue_order

监控曲线：[这里](https://t.elenet.me/dashboard/dashboard/db/system-monitor-rabbitmq-queues?from=1488432674328&to=1488454375317&var-vhost=zeus_fanout&var-name=osc_hestia_queue_order)

![osc_hestia_queue_order_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/osc_hestia_queue_order_1.png "osc_hestia_queue_order_1")

![osc_hestia_queue_order_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/osc_hestia_queue_order_2.png "osc_hestia_queue_order_2")

![osc_hestia_queue_order_3](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/osc_hestia_queue_order_3.png "osc_hestia_queue_order_3")

读图：

- 从 messages total 图中可以看到，消息从 14:20 左右开始积压，持续到 18:00 被 purge 掉，最高消息量 4.11 Mil；
- unacked 消息在上述时间段一直保持在 11.25k 左右；
- deliver 和 ack 的速度在上述时间段，只有 100～3k 左右，平均 1.12k msg/s；
- publish 速度平均为 2.02k ，在 17:57 分左右瞬间飙到 418k 左右；
- memory 在上述时间段，呈现周期性梯形上涨，最高涨到 4.47 Bil 左右；
- consumer 数量在上述时间段为 56 个；
- consumer util 在上述时间段为 0 ；

分析：

- `56 consumer * 200 prefetch_count = 11.2k unacked msg` ，说明 unacked 曲线是合理的（经确认，现在已改成了 500 prefetch_count）；
- `(2.02k publish rate - 1.12k delivery_get rate) * 3600 = 3.24 M` ，说明理论上（平均速度下）每小时消息堆积量会增加这么多，实际读图发现增量大概在 2M msg/h 左右；
- delivery_get 曲线和 ack 曲线的平均速率在 1.12k msg/s ，在压测时间段内存在 56 consumer ，说明**每个 consumer 在 1 秒时间内处理了 20 条消息**（本人意见如下：在当前配置下，满载时 "in flight" 消息数量为 11.2k ，而当前消费者处理能力基本在 20 msg/s 左右，说明 prefetch_count 的配置数值已经超过了 consumer 的处理能力，此时再增加其数值不会再有效果；另外，超过 consumer 处理能力的 prefetch count 数值并不会对 consumer 造成什么影响，因为对于 consumer 来说不过是多缓存了几条消息而已；因此结论就是：业务 consumer 的处理能力已经满载，若想进一步提升消息处理速度，则需要优化 consumer 的消息处理模型；**具体情况需要业务确认是否合理**）；
- 其他分析结论同上；


## vhost:zeus_fanout + queue:base_openapi_queue_order

监控曲线：[这里](https://t.elenet.me/dashboard/dashboard/db/system-monitor-rabbitmq-queues?from=1488432674328&to=1488454375317&var-vhost=zeus_fanout&var-name=base_openapi_queue_order)

![base_openapi_queue_order_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/base_openapi_queue_order_1.png "base_openapi_queue_order_1")

![base_openapi_queue_order_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/base_openapi_queue_order_2.png "base_openapi_queue_order_2")

![base_openapi_queue_order_3](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/base_openapi_queue_order_3.png "base_openapi_queue_order_3")

读图：

- 从 messages total 图中可以看到，消息从 14:20 左右开始积压，持续到 16:44 被 purge 掉，最高消息量 4.18 Mil；
- unacked 消息在上述时间段一直保持在 1k 左右，在 15:12 和 16:31 有两个毛刺分别达到 2.7k 和 7.1k ；在 16:36 之前为 1k 左右，之后为 2k 左右；
- deliver 和 ack 的速度在上述时间段，只有 100～3k 左右，在 15:19 和 17:56 有两个毛刺，平均 1.16k；
- publish 速度平均为 1.98k ，在 17:57 分左右瞬间飙到 419k 左右；
- memory 在上述时间段，呈现周期性梯形上涨，最高上涨到 3.72 Bil 左右，回落位置在 140 Mil 左右；
- consumer 数量在 16:36 之前为 5 个；之后为 10 个；
- consumer util 在上述时间段为 0 ；

分析：

- `5 consumer * 200 prefetch_count = 1k unacked msg` ，说明 unacked 曲线是合理的；
- `(1.98k publish rate - 1.16k delivery_get rate) * 3600 = 2.95 M` ，说明理论上（平均速度下）每小时消息堆积量会增加这么多，实际读图发现增量大概在 4.24M msg/h 左右；
- delivery_get 曲线和 ack 曲线的平均速率在 1.16k msg/s ，在压测时间段内存在 5 consumer ，说明**每个 consumer 在 1 秒时间内处理了 232 条消息**（本人意见如下：在当前配置下，满载时 "in flight" 消息数量为 1k ，而当前消费者处理能力基本在 232 msg/s 左右，说明 prefetch_count 的配置值略低，但基本已经达到比较好的配置值；因此结论就是：业务 consumer 的处理能力被当前配置略微限制了；**具体情况需要业务确认是否合理**）；
- 从 memory 曲线可以看出，该 queue 的内存上涨速度更快，触发 GC 的频率更高（匹配消息积压速率情况）；
- 其他分析结论同上；

## vhost:zeus_fanout + queue:osc_bacchus_queue

> 之前有业务开发专门针对该 queue 提出过质疑；

监控曲线：[这里](https://t.elenet.me/dashboard/dashboard/db/system-monitor-rabbitmq-queues?var-vhost=zeus_fanout&var-name=osc_bacchus_queue&from=1488433036183&to=1488456037610)

![osc_bacchus_queue_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/osc_bacchus_queue_1.png "osc_bacchus_queue_1")

![osc_bacchus_queue_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/osc_bacchus_queue_2.png "osc_bacchus_queue_2")

![osc_bacchus_queue_3](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/osc_bacchus_queue_3.png "osc_bacchus_queue_3")

读图：

- 从 messages total 图中可以看到，消息从 14:20 左右开始偶尔发生短暂积压，持续到 19:00 左右；最高积压消息量 29.52K ；
- unacked 曲线在上述时间段形状同 messages total 和 messages ready；
- deliver 和 ack 的速度在上述时间段，平均 183；
- publish 速度平均为 289 ，在 17:57 分左右瞬间飙到 54.6k 左右；
- memory 在上述时间段，呈现的是脉冲性上涨，最高才 201.46 Mil 左右，能够回落到 0 ；
- consumer 数量在上述时间段内为 16 个；
- consumer util 在上述时间段为 0 ；

分析：

- `16 consumer * 200 prefetch_count = 3.2k unacked msg` ，从 unacked 曲线中可以看到，最大值基本就是 3.2k ，并且大部分时间 queue 中无积压消息，因此 unacked 曲线为 0 是合理的；
- 该 queue 没有消息堆积问题；
- delivery_get 曲线和 ack 曲线的峰值速度为 2.82k 左右；由于曲线不连续，因此平均速度没有什么太大意义；在压测时间段内存在 16 consumer ，说明**每个 consumer 在 1 秒时间内处理了 176 条消息**（个人意见如下：consumer 尚未满载；**需要业务确认是否为合理情况**）；
- 从 memory 曲线可以看出，由于没有积压消息问题，内存使用波动属于正常范围；
- 其他分析结论同上；

# 问题日志

针对3月2日 RabbitMQ 日志的分析详见[这里](https://github.com/moooofly/MarkSomethingDown/blob/master/RabbitMQ/2017-03-02%20%E7%BA%BF%E4%B8%8A%20RabbitMQ%20%E5%BC%82%E5%B8%B8%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90.md)；

> 这个错误和上述问题应该没有直接关系；

# 问题总结

从这次事件中，可以发现如下几个问题：

- 消息积压达到一定量级后（这个量级和消息的大小是直接相关的，因此单纯说 100w+ 消息就需要进行 purge 其实并不严谨），会触发 GC ，进而对 produer 和 consumer 产生各种影响；
- 业务 consumer 的消息处理能力，并没有进行过严格确认和调优，而这个确认和调优恰恰只有业务自己才能完成（目前据我所知，consumer 数量和 prefetch count 的数值基本上是拍脑袋给出的）；
- 部分指标还需要完善（比如 prefetch count 的数值对于排查问题是有意义的，但由于客观原因，我们将 xg-fanout-rmq-* 集群的 web 控制台关闭了，因此为了获得该数值绕了一大圈；~~另外上面图中的 consumer util 的指标都为 0 明显存在问题~~；经确认，RabbitMQ 3.2.4 版本中尚未提供相应功能）；
- 在复盘中，若遇到涉及 rmq 问题的部分，理论上讲，都需要将生产方，消费方交代清楚；如果涉及到多级 rmq 使用，则更应交代清楚；目前的现状是，业务在发现问题后会基于自己的分析理解给出一些“rmq 存在xx问题”的结论，然后会在无法得到进一步结论的时候，才想起来叫 rmq 专家过来；在叫过来后，往往是前因后果没交代清楚就让查问题，orz..；
- 业务需要严格对待遇到的每个错误信息：例如，在3月2日的 RabbitMQ 日志中发现爆出 N 多 frame_error 错误，经[排查](https://github.com/moooofly/MarkSomethingDown/blob/master/RabbitMQ/2017-03-02%20%E7%BA%BF%E4%B8%8A%20RabbitMQ%20%E5%BC%82%E5%B8%B8%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90.md)，基本确认是 pika 本身的 bug 导致，和业务人员交流时发现，居然早就在业务日志中遇到相应的错误信息，但没有进行处理；orz...，纵使 RabbitMQ 非常稳定也不带这么玩的吧，估计换成别的什么 Q 早报废了；
- 关（jing）于（gao）最（ge）佳（wei）实（da）践（ye）：RabbitMQ 真的不是用于做消息持久化存储的，而是作为消息中间件进行消息转发的（这也是为什么 queue 为空的情况下，转发效率最高）；RabbitMQ 为了保证**消息的可靠性**，提供了持久化存储机制；为了避免在异常使用场景下 RabbitMQ 被搞崩溃，进而影响其他业务使用，提供了**流控机制**；因此，我们需要按照推荐的方式来使用（这个问题本质上与“仅将 Redis 作为缓存而非持久化存储”是一样一样的）；
- 最后一个问题：某个 RabbitMQ 节点上存在 A 和 B 两个 queue ，A queue 由于“某种不可抗拒的外部力量”堆积了 N Bil 消息，B queue 正常使用；由于 A queue 消息堆积过量反复触发 GC ，问：此时 B 是否会受到影响？答：我认为应该会有影响（但我目前无法说清楚这个问题）；然而从上面的分析过程中可以知道，queue `pcd_marketing_queue_order_action` 和 `osc_bacchus_queue` 均跑在 `rabbit@xg-fanout-rmq-5` 这个节点上，前者的消息积压似乎并未对后者造成什么影响（具体分析上面已给出）；


# 优化方案

> 以下内容梳理自各大牛给出的最佳实践

- 在 consumer 处理能力满足的情况下，应该将 `prefetch` 尽量调大；
- 建议 consumer 采取一次 ack 多条消息的实现方式，而不是每条消息单独进行 ack（会增加一定的业务处理复杂度，目前业务应该都是每条消息 ack 一次）；
- 强烈建议以“军规”形式确保 queue 中积压的消息尽量少（在压测场景下应该以其他方式进行规约）；
- 如果将消息积压在 RabbitMQ 中就是使用目的，那么至少要使用 Lazy Queues 方案（从 RabbitMQ 3.6.0 版本开始支持）；
- 如果业务 producer 需要感知 RabbitMQ 触发资源阈值限制（内存或磁盘）后的阻塞通知，则需要实现针对该功能的 AMQP 协议扩展；
- 针对 **persistent** 消息路由到 **durable** queue 的情况，如果业务使用了 Publisher confirm 机制，那么 `basic.ack` 是在消息被持久化到 disk 之后发送的；而 RabbitMQ 的消息存储模块，或者按照一定的时间间隔（几百毫秒）批量持久化消息的（为了最小化 fsync(2) 的调用次数），或者当 queue 空闲时处理；这就意味着，在恒定的负载压力下，针对 `basic.ack` 的延迟可达几百毫秒；为了改进吞吐量，强烈建议应用要异步处理 acknowledgements 消息（按照流的方式处理），或者批量 publish 消息（消息打包），之后再等待 confirms ；
- 消息大小越小越能降低延迟，提高消息速率；而将小消息合并成大消息进行处理，能够增大吞吐量；
- 对消息进行高效的表达（更精简的数据格式 or 压缩），能够提升传输性能（业务处理复杂度略微提高）；
- 启用 `HiPE` 对性能提升有好处（在 **Erlang R17** 后 `HiPE` 已经相当稳定，据说有 2～3 倍提升）；
- 禁止使用事物，可靠性通过其他方案解决；
- 针对 queue 的最大长度，既可以基于消息数量进行限制，也可以基于总字节数进行限制（由所有消息 body 的长度之和决定，忽略消息属性占用的长度和任何其他额外开销），或者基于两种方式一起进行限制；
- 需要注意：内存阈值的设置值并不会真正阻止 RabbitMQ 使用超过该值的内存量，而只是一个会令 publisher 开始被阻塞的点；在最坏的情况下，由于 Erlang 垃圾回收器的原因，可能会导致内存使用量被 double（默认情况下为 RAM 的 80%）；因此强烈建议开启操作系统本身的 swap 或 page 功能；


