# 系统监控说明

## CPU Load 和 CPU Utilization

详见：《[CPU Utilization 和 CPU Load Average](https://github.com/moooofly/MarkSomethingDown/blob/master/Linux/CPU%20Utilization%20%E5%92%8C%20CPU%20Load.md)》

若进程创建过多线程（上万），也会导致系统慢的一塌糊涂，但可能没有什么 CPU 使用，内存也没什么占用，IO 也没有多少，但 load 非常高；

## CPU steal

- 由于服务商在提供虚拟机时存在 CPU 超卖问题，因此和其他人共享 CPU 使用的情况是可能的；
- 当发生 CPU 使用碰撞情况时，CPU 的使用取决于调度优先级；优先级低的进程会出现一点点 steal 值；若 steal 出现大幅升高，则说明由于超卖问题，把主机 CPU 占用满了；
- 不使用虚拟机时一般不用关心该指标；使用虚拟机时，steal 代表超卖的幅度，一般不为 0 ；

## CPU softirq 和 CPU IRQ

- 核心：发生中断时会打断当前“指令流水线”去处理中断；
- 中断花费时间越长，代码被打断程度越严重；
- 中断越频繁，一般来说上下文切换越频繁（每次中断都要发生很多切换）；
- 中断越频繁，中断消耗的 CPU 时间越长；
- IRQ 多说明被物理设备打断的次数越多；一般来说，主要是由于网卡上包量非常大导致（网卡 pps 越高，中断数越大，导致软硬中断所要求的 CPU 处理时间越长，导致性能下降）；因为在服务器上一般不会有那么多其它硬件事件发生；
- IRQ 达到一定值后一定会导致用户进程调度被延迟，没有办法避免；
- 需要关注中断发生在哪颗 CPU 上；大部分中断都是发生在 0 核心上，50% 左右；
- 目前使用 Intel 网卡（替代了原来的 Broadcom 网卡）为多队列网卡，即能中断到多少 CPU 核心就中断到多少；但事实上，并不会全部占用（一般为前xx个）；
- CPU 节能功能会导致 Linux 在调度时尽量将所有任务往一个 CPU 核心上调度；副作用就是可能导致你的目标任务发生调度延迟；
- 包量导致的延迟计算公式：100w pps --> 中断 100w 次 --> 延迟 50ms（每次中断的上下文切换时间成本为 50ns）
- 基于 DMA 解决小包问题只是优化策略，并不解决根本问题；
- pps 上升，也会造成 softirq 的上升，虽然内核调度策略会尽量将 softirq 往空核上调度，但空核不足时，一样处理不了；
- 网卡限制：缓冲区大小限制，最低 8k ，最高不确定；缓冲区大小影响 DMA 聚合的极限，超过极限会导致丢包；

linux 将中断分为两部分：

- 硬中断：进入**底半部**中断处理过程，完成一些和硬件直接相关的中断处理过程，例如按键；
- 软中断：进入**顶半部**中断处理过程，完成相应的处理流程，例如按键后的处理流程；


## Cache 和 Dirty

- 尚未（操作系统层面）被 flush 的写入数据，可以被读取，对应 dirty cache ；
- C 语言中的 flush 是针对缓存在进程中的数据的（只有程序执行 flush 后，数据才从进程里“出去”，别的进程才能看到），即 C 调用 flush 后不代表数据落盘，还需要调用 sync 进行落盘；
- 若在内核 sync 之前修改了写入内容，此时的状态为“数据已写入系统”，对应 dirty cache 概念，若量极大，说明有大量数据堆积在内存里有丢失风险，可能原因是硬盘写入速度太慢；

cache 和 buffer

- 可以近似认为是一样的东西；
- cache 对应块对象，底层是 block 结构，4k 级别；
- buffer 对应文件对象，底层是 dfs 结构；
- 可以粗略认为 cache + buffer 即总的缓存池；

## Slab

内核使用的所有内存片：

- 可回收 reclaimable
- 不可回收 unreclaimable

历史 bug：内核中使用了过多的内存片，导致内存大量被内核占用，应用无法得到所需要的内存，进而导致 IO 性能下降；

## Memory Util

```
used = total - free - buffer - cache - slab reclaimable
util = used / total
```

> slab 统计信息已被计算到内存使用情况里；

- 经验值：如果 util 超过 50% 则认为是有问题的（但并不一定导致问题）；
- 可能的问题原因，超过 50% 后，可用部分不足，会导致 IO 性能下降；
- 若是 IO 密集型的应用，在 util 超过 50% 后一定要注意；若非 IO 密集型，则内存使用 util 可以更高，例如 redis ，即使达到 90% 也没问题（纯内存用法）；
- 内存占用看 Top 3 即可；

## Swap

- 应该关掉；
- 1G 的 swap 需要 10s ；
- 在开了 1G swap 的情况下，若系统出现了 OOM 情况时，会导致 10s 内什么响应都没有；
- 100m swap 对系统的帮助不大；
- 若有些数据库必须要使用 swap ，当 swap usage 一直在涨时，就是有问题了；若存在 swap ，则应该一直保持在低值，一直涨一定有问题；


## Disk 

- storage util: 用来关注磁盘是否满了；
- storage inode util: 小文件过多，导致 inode 耗光；
- device/disk util: 磁盘总 IO 量和理论上可进行 IO 量的比值；一般来说，util 越高，IO 时间越长，IO 压力越大；有时该指标需要结合 IO 个数一起分析；
- 一个盘，在 1s 内有 50% 的时间在做 IO ，则 disk util 为 50% ；
- 当 disk util 达到 100% 时，表示的不是 IO 性能低，而是 IO 需要排队，此时 CPU 使用看起来是狂掉的（下跌）；此时 CPU 的 iowait 会升高；


机械盘 IOPS 为 100-500 ；
SSD 的 IOPS 为 几千 ；


## network

Network - In  进数据量
Network - Out 出数据量

Network Util - In  网络当前进流量/网络理论总流量值
Network Util - Out 网络当前出流量/网络理论总流量值

Network Speed 会由于 bond 显示为 2G ；

虚拟机没有 Network Speed 这个值，因此也就算不出 Network Util ；


## TCP

- `FIN_WAIT_1` 出现表示对端系统不正常，非应用不正常，因为 ack 是 TCP 协议栈自动回复的；`FIN_WAIT_1` 会在 75s 后自动消失；
- `FIN_WAIT_2` 出现说明对方处于 `CLOSE_WAIT` 的不正常状态，不会被自动回收，对应 orphans socket；系统会在 orphans 数量超限后找最老的干掉；处于 FIN_WAIT_2 状态的 socket 会一直卡在这个状态，容易产生业务问题；
- `TIME_WAIT` 关注业务行为是否符合预期；
- `CLOSE_WAIT` 可以直接怀疑业务有问题；
- retransmit segs 指标对于丢包、对方性能不足，网卡性能不足等问题，非常敏感；这个值应该在 0～1% 上下浮动（经验值）；
- 线上丢包率应该在万分之一到万分之三（自然背景丢包率），上限为 1%；
- RTO 也敏感，但面板上没有；表示和对端之间的平均响应延迟；

重传率浮动的原因：随着发包数目上升，报文发送速度越快，中间丢包概率就越大，甚至将中间网络设备打满，进而导致丢包；丢包又会导致发包速度降低，丢包可能又会随着降低；所以才会浮动；

丢包率问题对打开了 bbr 的机器不适合，该算法不会影响丢包率，对于开了 bbr 得机器，哪怕链路用满，丢包率也不会上升；线上默认为 cubic ；














