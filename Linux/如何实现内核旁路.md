# 如何实现内核旁路

> 原文地址：[这里](https://blog.cloudflare.com/kernel-bypass/)

不幸的是，对于一些更特别的工作负载，Vanilla Linux 内核的网络处理速度是不够的。举个例子，在 CloudFlare，我们持续地处理洪水般的数据包。**Vanilla Linux 处理速度仅能达到约 1M pps** ，这在我们的工作环境下是不够的，尤其时网卡是有能力处理大量数据包的。**现代 10Gbps 网卡的处理能力通常能够达到至少 10M pps** 。

![hispeed](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/hispeed.jpg "hispeed")

很明显，能够在我们已有硬件基础上，“挤压”出更多 packets 处理能力的办法就是从 Linux 内核网络协议栈下手，即 "kernel bypass" 技术；本文就深入研究实现该技术的各种方式；

## 内核不给力

可以通过一个小试验令你确信解决 Linux 问题确实是必要的；首先我们看看在完美条件下，kernel 能够处理多少 packets ；因为传递 packets 到用户空间代价高昂，因此替代方案为一旦 packets 离开了 network driver 代码的处理范围，就直接丢弃；据我所知，Linux 上不修改内核源码前提下，丢弃数据包的最快方法是：在 `PREROUTING` iptables chain 上设置一些 DROP 规则。

```
$ sudo iptables -t raw -I PREROUTING -p udp --dport 4321 --dst 192.168.254.1 -j DROP
$ sudo ethtool -X eth2 weight 1
$ watch 'ethtool -S eth2|grep rx'
     rx_packets:       12.2m/s
     rx-0.rx_packets:   1.4m/s
     rx-1.rx_packets:   0/s
     ...
```

> 注意：实际操作现实结果有所不同；

如上所示，网卡接收速度可达 12M pps ；通过 `ethtool -X` 来调整网卡上的间接表（indirection table），可以将所有的数据包引向 RX queue #0 ；正如我们看到的，kernel 能够在单 CPU 的一个 queue 上达到 1.4M pps 的处理速度；

在单核上达到 1.4M pps 是一个相当不错的结果，但不幸的是，协议栈无法进行扩展。当数据包被分配到多核上时，这个成绩会急剧下降。让我们看看把数据包分到 4 个 RX queues 的结果。

```
$ sudo ethtool -X eth2 weight 1 1 1 1
$ watch 'ethtool -S eth2|grep rx'
     rx_packets:     12.1m/s
     rx-0.rx_packets: 477.8k/s
     rx-1.rx_packets: 447.5k/s
     rx-2.rx_packets: 482.6k/s
     rx-3.rx_packets: 455.9k/s
```

此时每个核的处理速度是 480k pps。这是个糟糕的消息。即使乐观地假设增加多个核心不会进一步地造成性能的下降，处理数据包的核心也要多达 20 个才能达到线速度。所以内核是不起作用的。

## 内核旁路前来救驾

![bypass-1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/bypass-1.jpg "bypass-1")

关于 Linux 内核网络性能的局限早已[不是什么新鲜事](https://lwn.net/Articles/629155/)了。在过去的几年中，人们多次尝试解决这个问题。这里列出一些广为人知的内核旁路技术。

### PACKET_MMAP

[Packet_mmap](https://www.kernel.org/doc/Documentation/networking/packet_mmap.txt) 是一种用于 fast packet sniffing 的 Linux API 。尽管它不是严格意义上的内核旁路技术，但它仍就在该列表中占有一席之地，该技术在 Vanilla kernels 中已经存在。

### PF_RING

PF_RING 是另一种用来提升数据包捕获速度的知名技术。与 `packet_mmap` 不同，PF_RING 不在内核主线中，并且需要特殊模块支持。通过 ZC drivers 和 `transparent_mode = 2` 设置，**所有的 packets 将只会被传递给 PF_RING 客户端，而不会投给内核网络协议栈**。由于内核正是导致 packet 处理变慢的原因，该技术确保了最快的处理。

### Snabbswitch

`Snabbswitch` 是一种基于 Lua 语言的网络框架，主要用来写 L2 应用。其工作方式为：完全接管网卡，并在用户空间实现了一种硬件驱动。其通过将设备寄存器 mmap 到 `sysfs` 上（sysfs 是 Linux 内核中设计较新的一种虚拟的基于内存的文件系统），在 PCI 设备级别上实现了[用户空间 IO (UIO)](https://lwn.net/Articles/232575/)；这种方式实现了非常快地操作能力，但是也导致**数据包完全跳过了内核网络协议栈**。

### DPDK

`DPDK` 是一个基于 C 语言实现的网络框架，专门为 Intel 芯片所创造。本质上和 `snabbswitch` 类似，因为它也是一套完整框架，并依[赖于 UIO](https://www.slideshare.net/garyachy/dpdk-44585840) 。

### Netmap

`Netmap` 也是一个功能丰富的网络框架，但和 UIO 技术相比，其最终实现为多个内核模块；为了和网络硬件集成在一起，用户需要给内核网络驱动打补丁。增加复杂性的最大好处是有一个详细文档说明的、[设备厂商无关的和清晰的 API](https://www.freebsd.org/cgi/man.cgi?query=netmap&sektion=4) 。


## 方案选择

由于内核旁路技术的主要目的就是不再让内核处理数据包，所以我们可以排除 `packet_mmap` 方式。因为这种方式不会接管数据包，其只是一个用于数据包嗅探的快速接口。同样，不支持 ZC 模块的普通 `PF_RING` 也没有什么吸引力，因为其主要目标是对 libpcap 进行加速（libpcap 是 unix/linux 平台下的网络数据包捕获函数包，大多数网络监控软件都以它为基础）。

我们已经排除了两种技术，但不幸的是，对于我们遇到的工作负载情况，[剩余的解决方案](https://www.slideshare.net/shemminger/uio-final)中同样没有一种满足要求；


让我解释下为何这么说；为了实现内核旁路技术，剩余的技术：`Snabbswitch`、`DPDK` 和 `netmap` 需要接管（独占）整个网卡，不允许该网卡上的任何 traffic 到达（经过）内核。在 CloudFlare 中，我们不能接受让一个 offloaded 应用**独占整个网卡**。

不得不说，有很多人会使用上面的技术；在其他条件允许的情况下，独占一个网卡用于旁路可能是一个可以接受的方案。

## 无需独占网卡的技术

### Solarflare 之 EF_VI

![efvi-model](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/efvi-model.png "efvi-model")

Solarflare 网卡支持 OpenOnload，一个神奇的网卡加速器。EF_VI 作为一个专用库，仅能用在 Solarflare 网卡上；在底层，每个 EF_VI 程序可以访问一条特定的 RX 队列，这条 RX 队列对内核不可见的。默认情况下，这个队列不接收数据，直到你创建了一个 EF_VI “过滤器”。这个过滤器只是一个隐藏的流控制规则。你用 ethtool -n 也看不到，但实际上这个规则已经存在网卡中了。对于 EF_VI 来说，除了分配 RX 队列并且管理流控制规则，剩下的任务就是提供一个API 让用户空间可以访问这个队列。

### Bifurcated driver（分叉驱动）

![dpdk-model](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/dpdk-model.png "dpdk-model")

虽然 EF_VI 是 Solarflare 所特有的，其他网卡还是可以复制这个技术。首先我们需要一个支持多队列的网卡，同时它还支持流控制和操作间接表。

有了这些功能，我们可以：

- 正常启动网卡，让内核来管理一切。
- 修改间接表以确保没有数据包流向任一 RX 队列。比如说我们选择 16 号 RX 队列。
- 通过流控制规则将一个特定的网络流引到 16号 RX 队列。

完成这些，剩下的步骤就是提供一个用户空间的 API ，从 16 号 RX 队列上接收数据包，并且不会影响其他任何队列。

这个想法在 DPDK 社区被称为“分叉驱动”。它们打算在  2014 年创建分叉驱动，不幸的是 这个补丁 还没进入内核的主线。

### Virtualization approach（虚拟化方案）

![virt-model](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/virt-model.png "virt-model")

针对 intel 82599 还有另外一种选择。我们可以利用网卡上的虚拟化功能来实现内核旁路，而不需要通过分叉驱动程序。

首先我简单说下背景。有结果证明，在虚拟化世界中将数据包从主机传递到客户机，虚拟机通常是瓶颈。因此，这些年对虚拟化性能的需求与日俱增，通过软件模拟网络硬件的仿真技术成为了影响性能的主要障碍。

网卡厂商增加一些特性来加速虚拟客户端。其中一项虚拟化技术，要求网卡虚拟成多个 PCI 设备。虚拟客户端可以操作这些虚拟接口，无需与主机操作系统进行任何合作。

## 总结

实现内核旁路并没有那么简单。虽然已存在很多开源技术，但看起来全部都需要一块专用（dedicated）网卡。这里我们展示三种可以选择的框架：

- EF_VI-style hidden RX queues
- DPDK bifurcated driver
- The VF hack

不幸的是，在我们所研究过的众多技术中，似乎只有 EF_VI 满足我们的实际环境。我热切期望一种开源内核旁路 API 的出现，唯一的要求是不需要一块专用的网卡。