

# 问题描述

- 发生问题的时间段：**14:13:45 -- 15:09:00** 左右；
- 发生问题的机器：整个 RabbitMQ cluster 都受到影响，但 qc-moses-rmq-bak-7 节点对应的虚拟机发生了异常；

> 据说在上述时间段，腾讯云有部分机器出现异常（或者直接 down 机，或者网络不可达），而发生问题的 RabbitMQ 集群正好在这批故障机器范围内；

# 环境信息

- RabbitMQ cluster 节点运行在腾讯云虚拟机上；
- RabbitMQ 版本为 3.2.4 ；
- 存在 queue sharding 策略；存在持久化 queue ；

# 事故现象

问题发生时，正好是 mesos 业务推单时间段（业务高峰期）；故障发生后，看到业务曲线（具体是什么曲线我不清楚）和 RabbitMQ 监控曲线掉底；

## 系统指标

9 节点 cluster 整体情况
![CPU 使用情况_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016-08-17%20腾讯云网络故障%2Brabbitmq节点7异常_1.png "CPU 使用情况_1")

节点 7 情况
![CPU 使用情况_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016-08-17%20腾讯云网络故障%2Brabbitmq节点7异常_2.png "CPU 使用情况_2")

节点 7 和 9 对比情况
![CPU 使用情况_3](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016-08-17%20腾讯云网络故障%2Brabbitmq节点7异常_3.png "CPU 使用情况_3")

## TCP 指标

节点 7 上 TCP 连接数量变化
![TCP 连接情况_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016-08-17%20腾讯云网络故障%2Brabbitmq节点7异常_4.png "TCP 连接情况_1")

节点 7 和 9 上 TCP 连接数量变化对比
![TCP 连接情况_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016-08-17%20腾讯云网络故障%2Brabbitmq节点7异常_5.png "TCP 连接情况_2")

节点 1,2,4,5,6,9 上 SYN->ACK 超时情况（问题开始时刻：**14:13:45** ，问题恢复时刻：**14:27:30**）
![TCP SYC_ACK 超时_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/124569_tcp_ack_timeout.png "TCP SYC_ACK 超时_1")

节点 3,8 上 SYN->ACK 超时情况（问题开始时刻：**14:13:45** ，问题恢复时刻：**15:09:00**）
![TCP SYC_ACK 超时_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/38_tcp_ack_timeout.png "TCP SYC_ACK 超时_2")

节点 7 上 SYN->ACK 超时情况（⚠️ 该曲线比较特殊：在 **14:13:45** 前，曲线为 0 表示网络正常；从 **14:13:45** 到 **15:09:00** 这段时间，由于网络故障＋运维切换等行为，此时并未得到真实数据，而是沿用了之前的数据；在 **15:09:00** 之后，曲线为 0 表示网络正常）
![TCP SYC_ACK 超时_3](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/7_tcp_ack_timeout.png "TCP SYC_ACK 超时_3")

## RabbitMQ 指标

节点 7 上 RabbitMQ 各项指标（从曲线可以看出，该节点的各项信息都是突然中断的，怀疑可能原因：RabbitMQ 服务突然挂掉 or RabbitMQ 运行的虚拟机突然挂掉）
![rmq_bak_7 曲线_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/rmq_bak_7%20曲线_1.png "rmq_bak_7 曲线_1")

![rmq_bak_7 曲线_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/rmq_bak_7%20曲线_2.png "rmq_bak_7 曲线_2")

节点 4 上 RabbitMQ 各项指标（在节点 7 异常的时刻，曲线同样立刻掉底）
![rmq_bak_4 曲线_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/rmq_bak_4%20曲线_1.png "rmq_bak_4 曲线_1")

（曲线中空白的一段对应运维切换集群的操作）
![rmq_bak_4 曲线_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/rmq_bak_4%20曲线_2.png "rmq_bak_4 曲线_2")

# 事故分析

对比分析上述所有监控曲线，发现只有 RabbitMQ 节点 7 的比较特殊，即无法获取任何数据导致；其它节点在掉底时仍然可以获取到数据；因此可以推断：RabbitMQ 节点 7 要么存在网络问题，要么就是节点所在的虚拟机存在问题，要么就是 RabbitMQ 节点自身存在问题；

后经运维证实：出现问题时，通过 ssh 无法登陆 RabbitMQ 节点 7（其它节点可以登录），通过腾讯云管理面板重启相应的虚拟机失败；后续能够连上该节点时（网络恢复后），发现 RabbitMQ 节点 7 进程已经不存在，且虚拟机 uptime 表明发生过重启；

经业务开发证实：业务如果基于 RabbitMQ 创建某个资源失败（具体是什么我不清楚），则不会继续后续业务推送行为；由上面的信息可以推断：由于网络故障，业务无法创建某资源，之后便停止了推送行为，进而导致业务曲线掉底；


# 事故发生时的运维救急处理

出现问题时的处理步骤：
- 尝试通过 ssh 连接到 RabbitMQ 节点 7 失败；
- 通过腾讯云控制面板重启对应的虚拟机失败；
- 通过 RabbitMQ 命令将节点 7 从集群中手动踢出（之后将 7 中的 queue 和 exchange 配置导入当前集群）；但踢出 7 后，业务掉底情况仍未得到改善（当时不清楚业务逻辑，也没注意到 SYN->ACK 超时情况，后续分析时怀疑，业务创建资源的节点可能就是 3 或 8 节点，即一直处于超时状态的两个节点）；
- 切换到备用集群，业务曲线恢复正常；

# 事故中的一些问题点

## 节点 1 上看到的信息

- 持久队列无法创建

```shell
=ERROR REPORT==== 17-Aug-2016::14:26:45 ===
connection <0.9888.4921>, channel 2 - soft error:
{amqp_error,not_found,
            "home node 'rabbit@qc-moses-rmq-bak-7' of durable queue 'base.moses.task4' in vhost 'moses' is down or inaccessible",
            'queue.declare'}

```
> 该错误是比较典型的 cluster 中的持久 queue 在所属节点挂掉后无法重新创建的问题；

- queue 和 exchange 未绑定

```shell
=ERROR REPORT==== 17-Aug-2016::14:42:38 ===
connection <0.10842.4927>, channel 1 - soft error:
{amqp_error,not_found,
            "no binding api.base.moses.task4 between exchange 'zeus' in vhost 'moses' and queue 'base.moses.task4' in vhost 'moses'",
            'queue.bind'}
```
> 该错误和上述错误有关联，即通过人为方式重新导入 queue 和 exchange 信息后，还需要主动创建 binding 关系；

- 网络超时错误

```shell
=ERROR REPORT==== 17-Aug-2016::14:14:39 ===
closing AMQP connection <0.16788.3104> (10.0.241.185:45303 -> 10.0.242.241:5672):
{inet_error,etimedout}
```


## 节点 7 上看到的信息

![RabbitMQ log 中的 hole](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/由于虚拟机异常导致的log中的hole.png "RabbitMQ log 中的 hole")

```shell
=ERROR REPORT==== 17-Aug-2016::14:13:32 ===
error on AMQP connection <0.27856.4894>: enotconn (socket is not connected)
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@
=INFO REPORT==== 17-Aug-2016::15:12:46 ===
Error description:
   {error,{inconsistent_cluster,"Node 'rabbit@qc-moses-rmq-bak-7' thinks it's clustered with node 'rabbit@qc-moses-rmq-bak-9', but 'rabbit@qc-moses-rmq-bak-9' disagrees"}}

Log files (may contain more information):
   /data/log/rabbitmq/rabbit@qc-moses-rmq-bak-7.log
   /data/log/rabbitmq/rabbit@qc-moses-rmq-bak-7-sasl.log

Stack trace:
   [{rabbit_mnesia,check_cluster_consistency,0,[]},
    {rabbit,'-boot/0-fun-1-',0,[]},
    {rabbit,start_it,1,[]},
    {init,start_it,1,[]},
    {init,start_em,1,[]}]
```

> 经过高人提点，上述 “^@“ 的出现应该和基于 lseek 创建“空洞文件“有关；

该文件的内容从另一个角度确认了如下结论：
- 腾讯云虚拟机发生了异常；
- RabbitMQ 本身并没有发生优雅的重启；


## 其它症状：

- 通过 rabbitmqctl status 看到 memory 中的 plugin 对应值为负数；

![status 中 memory plugins 为负数](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/rabbitmqctl%20status%20输出中%20plugins%20为负数.png "status 中 memory plugins 为负数")

- RabbitMQ 管理页面无法打开；
- 通过 rabbitmqctl eval 'application:stop(rabbitmq_management), application:start(rabbitmq_management).' 重启 management 时，发生卡住情况，一段时间后直接通过 ctrl＋c 终止该命令执行；


----------


节点 1 上的关键日志：

```shell
=INFO REPORT==== 17-Aug-2016::14:14:16 ===
rabbit on node 'rabbit@qc-moses-rmq-bak-7' down

=ERROR REPORT==== 17-Aug-2016::14:14:22 ===
error on AMQP connection <0.20289.4916>: enotconn (socket is not connected)

=ERROR REPORT==== 17-Aug-2016::14:14:39 ===
closing AMQP connection <0.16788.3104> (10.0.241.185:45303 -> 10.0.242.241:5672):
{inet_error,etimedout}

=ERROR REPORT==== 17-Aug-2016::14:15:12 ===
connection <0.24516.4924>, channel 1 - soft error:
{amqp_error,not_found,
            "home node 'rabbit@qc-moses-rmq-bak-7' of durable queue 'base.moses.task4' in vhost 'moses' is down or inaccessible",
            'queue.declare'}

=ERROR REPORT==== 17-Aug-2016::14:15:13 ===
connection <0.10856.4925>, channel 1 - soft error:
{amqp_error,not_found,
            "home node 'rabbit@qc-moses-rmq-bak-7' of durable queue 'base.moses.task4' in vhost 'moses' is down or inaccessible",
            'queue.declare'}

=ERROR REPORT==== 17-Aug-2016::14:15:38 ===
closing AMQP connection <0.26582.4898> (10.0.241.8:44857 -> 10.0.242.241:5672):
{inet_error,etimedout}

=ERROR REPORT==== 17-Aug-2016::14:15:38 ===
closing AMQP connection <0.13841.4928> (10.0.241.8:44916 -> 10.0.242.241:5672):
{inet_error,etimedout}

=ERROR REPORT==== 17-Aug-2016::14:15:38 ===
closing AMQP connection <0.6287.4905> (10.0.241.8:45153 -> 10.0.242.241:5672):
{inet_error,etimedout}

=ERROR REPORT==== 17-Aug-2016::14:15:38 ===
closing AMQP connection <0.32292.4917> (10.0.241.8:44762 -> 10.0.242.241:5672):
{inet_error,etimedout}
```

# 腾讯云给出的解释

腾讯云当机是因为灰度发版触发内核 bug 导致宿主机失联；

![腾讯云故障报告](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016年8月17日%20腾讯云故障报告.png "腾讯云故障报告")


# 小结

- 不了解业务问题很难查；
- 不会分析曲线问题很难查；
- 不了解 RabbitMQ 各项指标问题很难查；

网络问题很容易导致各种奇怪问题的发生，腾讯云为我们提供了很好的实验场所！！

本次事件的意义：
- RabbitMQ 曲线掉底很可能是由业务自身行为导致的；
- 整个机房的网络故障（每个虚拟机节点的网络恢复有先后），容易让人造成现象误判，因为业务可能会和多个节点交互，存在部分能通，部分不通；面对网络故障的问题时，应该针对 cluster 中的所有节点进行连通性检测；
- 虚拟机层面的异常和 RabbitMQ 节点自身的异常似乎需要进一步研究测试；
- 异常节点的 cluster 踢出和相关 fabric 的导入处理还需要完善；