

# 问题描述

- 发生问题的时间段：**14:13:45 -- 15:09:00** 左右；
- 发生问题的机器：整个 RabbitMQ cluster 都受到影响，但 qc-moses-rmq-bak-7 节点对应的虚拟机发生了异常；

> 据说在上述时间段，腾讯云有部分机器出现异常（或者直接 down 机，或者网络不可达），而发生问题的 RabbitMQ 集群正好在这批故障机器范围内；

# 环境信息

- RabbitMQ cluster 节点运行在腾讯云虚拟机上；
- RabbitMQ 版本为 3.2.4 ；
- 存在 queue sharding 策略；

# 事故影响

问题发生时，正好是 mesos 业务推单时间段（业务高峰期）；故障发生后，看到业务曲线（具体是什么曲线我不清楚）和 RabbitMQ 监控曲线掉底；

## 系统指标

9 节点 cluster 整体情况
![CPU 使用情况_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016-08-17%20腾讯云网络故障%2Brabbitmq节点7异常_1.png "CPU 使用情况_1")

节点 7 情况
![CPU 使用情况_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016-08-17%20腾讯云网络故障%2Brabbitmq节点7异常_2.png "CPU 使用情况_2")

节点 7 和 9 对比情况
![CPU 使用情况_3](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016-08-17%20腾讯云网络故障%2Brabbitmq节点7异常_3.png "CPU 使用情况_3")

## TCP 指标

节点 7 上 TCP 连接数量变化
![TCP 连接情况_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016-08-17%20腾讯云网络故障%2Brabbitmq节点7异常_4.png "TCP 连接情况_1")

节点 7 和 9 上 TCP 连接数量变化对比
![TCP 连接情况_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/2016-08-17%20腾讯云网络故障%2Brabbitmq节点7异常_5.png "TCP 连接情况_2")

节点 1,2,4,5,6,9 上 SYN->ACK 超时情况
![TCP SYC_ACK 超时_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/124569_tcp_ack_timeout.png "TCP SYC_ACK 超时_1")

节点 3,8 上 SYN->ACK 超时情况
![TCP SYC_ACK 超时_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/38_tcp_ack_timeout.png "TCP SYC_ACK 超时_2")

节点 7 上 SYN->ACK 超时情况
![TCP SYC_ACK 超时_3](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/7_tcp_ack_timeout.png "TCP SYC_ACK 超时_3")

## RabbitMQ 指标

节点 7 上 RabbitMQ 各项指标
![rmq_bak_7 曲线_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/rmq_bak_7%20曲线_1.png "rmq_bak_7 曲线_1")

![rmq_bak_7 曲线_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/rmq_bak_7%20曲线_2.png "rmq_bak_7 曲线_2")

节点 4 上 RabbitMQ 各项指标
![rmq_bak_4 曲线_1](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/rmq_bak_4%20曲线_1.png "rmq_bak_4 曲线_1")

![rmq_bak_4 曲线_2](https://raw.githubusercontent.com/moooofly/ImageCache/master/Pictures/rmq_bak_4%20曲线_2.png "rmq_bak_4 曲线_2")


观察 RabbitMQ 监控曲线，发现只有 RabbitMQ 节点 7 的掉底是由于无法获取统计数据导致；其它节点在掉底时仍然可以获取到数据；结论：RabbitMQ 节点 7 要么存在网络问题，要么就是节点所在的虚拟机存在问题，要么就是 RabbitMQ 节点自身存在问题；

出现问题时，发现通过 ssh 无法登陆 RabbitMQ 节点 7 ，故当时无法确认具体原因；但后续能够连上时（网络恢复后），发现 RabbitMQ 节点 7 进程已经不存在（虚拟机重启过 or 节点异常）；

出现问题时，由于无法连接到 rabbitmq 节点 7 ，且业务取消掉底，故通过命令将节点 7 从集群中手动踢出；但踢出后，业务掉底情况仍未得到改善，故直接切换到备用集群，业务曲线恢复正常；

踢出节点 7 后，（后续查看）节点 1 的日志中发现存在两种错误：
- 持久队列无法创建

```shell
=ERROR REPORT==== 17-Aug-2016::14:26:45 ===
connection <0.9888.4921>, channel 2 - soft error:
{amqp_error,not_found,
            "home node 'rabbit@qc-moses-rmq-bak-7' of durable queue 'base.moses.task4' in vhost 'moses' is down or inaccessible",
            'queue.declare'}

```

- xx queue 和 xx exchange 未绑定



```shell
=ERROR REPORT==== 17-Aug-2016::14:42:38 ===
connection <0.10842.4927>, channel 1 - soft error:
{amqp_error,not_found,
            "no binding api.base.moses.task4 between exchange 'zeus' in vhost 'moses' and queue 'base.moses.task4' in vhost 'moses'",
            'queue.bind'}
```



网络错误


发现 7 断开
```shell
=INFO REPORT==== 17-Aug-2016::14:14:16 ===
rabbit on node 'rabbit@qc-moses-rmq-bak-7' down

=ERROR REPORT==== 17-Aug-2016::14:14:22 ===
error on AMQP connection <0.20289.4916>: enotconn (socket is not connected)

=ERROR REPORT==== 17-Aug-2016::14:14:39 ===
closing AMQP connection <0.16788.3104> (10.0.241.185:45303 -> 10.0.242.241:5672):
{inet_error,etimedout}
```

```shell
=ERROR REPORT==== 17-Aug-2016::14:15:12 ===
connection <0.24516.4924>, channel 1 - soft error:
{amqp_error,not_found,
            "home node 'rabbit@qc-moses-rmq-bak-7' of durable queue 'base.moses.task4' in vhost 'moses' is down or inaccessible",
            'queue.declare'}

=ERROR REPORT==== 17-Aug-2016::14:15:13 ===
connection <0.10856.4925>, channel 1 - soft error:
{amqp_error,not_found,
            "home node 'rabbit@qc-moses-rmq-bak-7' of durable queue 'base.moses.task4' in vhost 'moses' is down or inaccessible",
            'queue.declare'}
```

```shell
=ERROR REPORT==== 17-Aug-2016::14:15:38 ===
closing AMQP connection <0.26582.4898> (10.0.241.8:44857 -> 10.0.242.241:5672):
{inet_error,etimedout}

=ERROR REPORT==== 17-Aug-2016::14:15:38 ===
closing AMQP connection <0.13841.4928> (10.0.241.8:44916 -> 10.0.242.241:5672):
{inet_error,etimedout}

=ERROR REPORT==== 17-Aug-2016::14:15:38 ===
closing AMQP connection <0.6287.4905> (10.0.241.8:45153 -> 10.0.242.241:5672):
{inet_error,etimedout}

=ERROR REPORT==== 17-Aug-2016::14:15:38 ===
closing AMQP connection <0.32292.4917> (10.0.241.8:44762 -> 10.0.242.241:5672):
{inet_error,etimedout}
```


后续确认 rabbitmq 7 节点上存在两个持久队列；
后续观察备用集群的 rabbitmq 7 节点的消息 publish 和 deliver 速率，认为其并非消息处理的关键路径；进而认为节点 7 的消失不应该导致业务曲线的掉底；另外和业务开发确定了业务本身不会因为与节点 7 的断开而停止工作；


出现问题时的其它症状：
- 通过 rabbitmqctl status 看到 memory 信息中的 plugin 对应值为负数；
- 管理页面无法打开
- 通过 rabbitmqctl eval 'application:stop(rabbitmq_management), application:start(rabbitmq_management).' 重启 management 时，发生卡住情况，一段时间后直接通过 ctrl＋c 终止该命令执行；




节点 7 上的奇怪日志

```shell
=ERROR REPORT==== 17-Aug-2016::14:13:32 ===
error on AMQP connection <0.27856.4894>: enotconn (socket is not connected)
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@
=INFO REPORT==== 17-Aug-2016::15:12:46 ===
Error description:
   {error,{inconsistent_cluster,"Node 'rabbit@qc-moses-rmq-bak-7' thinks it's clustered with node 'rabbit@qc-moses-rmq-bak-9', but 'rabbit@qc-moses-rmq-bak-9' disagrees"}}

Log files (may contain more information):
   /data/log/rabbitmq/rabbit@qc-moses-rmq-bak-7.log
   /data/log/rabbitmq/rabbit@qc-moses-rmq-bak-7-sasl.log

Stack trace:
   [{rabbit_mnesia,check_cluster_consistency,0,[]},
    {rabbit,'-boot/0-fun-1-',0,[]},
    {rabbit,start_it,1,[]},
    {init,start_it,1,[]},
    {init,start_em,1,[]}]


=INFO REPORT==== 17-Aug-2016::15:10:48 ===
Error description:
   {error,{inconsistent_cluster,"Node 'rabbit@qc-moses-rmq-bak-7' thinks it's clustered with node 'rabbit@qc-moses-rmq-bak-9', but 'rabbit@qc-moses-rmq-bak-9' disagrees"}}

Log files (may contain more information):
   /data/log/rabbitmq/rabbit@qc-moses-rmq-bak-7.log
   /data/log/rabbitmq/rabbit@qc-moses-rmq-bak-7-sasl.log

Stack trace:
   [{rabbit_mnesia,check_cluster_consistency,0,[]},
    {rabbit,'-boot/0-fun-1-',0,[]},
    {rabbit,start_it,1,[]},
    {init,start_it,1,[]},
    {init,start_em,1,[]}]
```


经过高人提点，上述 “^@“ 的出现应该和 lseek 创建“空洞文件“有关；


昨天腾讯云当机是因为灰度发版触发内核bug导致宿主机失联。

